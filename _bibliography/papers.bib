---
---
@article{gupte2024revisiting,
  abbr={TMLR},
  title={Revisiting Active Learning in the Era of Vision Foundation Models},
  author={Sanket Rajan Gupte* and Josiah Aklilu* and Jeffrey J Nirschl and Serena Yeung-Levy},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2024},
  url={https://openreview.net/forum?id=u8K83M9mbG},
  note={},
  pdf={https://arxiv.org/pdf/2401.14555.pdf},
  abstract={Foundation vision or vision-language models are trained on large unlabeled or noisy data and learn robust representations that can achieve impressive zero- or few-shot performance on diverse tasks. Given these properties, they are a natural fit for active learning (AL), which aims to maximize labeling efficiency, but the full potential of foundation models has not been explored in the context of AL, specifically in the low-budget regime. In this work, we evaluate how foundation models influence three critical components of effective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling, and 3) the trade-off between representative and uncertainty sampling. We systematically study how the robust representations of foundation models (DINOv2, OpenCLIP) challenge existing findings in active learning. Our observations inform the principled construction of a new simple and elegant AL strategy that balances uncertainty estimated via dropout with sample diversity. We extensively test our strategy on many challenging image classification benchmarks, including natural images as well as out-of-domain biomedical images that are relatively understudied in the AL literature. Source code will be made available.},
  eprint={2401.14555},
  code={https://github.com/sanketx/AL-foundation-models},
  selected={true}
}

@misc{rau2024depthguided,
      abbr={ECCV},
      title={Depth-guided NeRF Training via Earth Mover's Distance}, 
      author={Anita Rau and Josiah Aklilu and F. Christopher Holsinger and Serena Yeung-Levy},
      year={2024},
      pdf={https://arxiv.org/pdf/2403.13206v1.pdf},
      abstract={Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of predicted viewpoints. However, the photometric loss often does not provide enough information to disambiguate between different possible geometries yielding the same image. Previous work has thus incorporated depth supervision during NeRF training, leveraging dense predictions from pre-trained depth networks as pseudo-ground truth. While these depth priors are assumed to be perfect once filtered for noise, in practice, their accuracy is more challenging to capture. This work proposes a novel approach to uncertainty in depth priors for NeRF supervision. Instead of using custom-trained depth or uncertainty priors, we use off-the-shelf pretrained diffusion models to predict depth and capture uncertainty during the denoising process. Because we know that depth priors are prone to errors, we propose to supervise the ray termination distance distribution with Earth Mover’s Distance instead of enforcing the rendered depth to replicate the depth prior exactly through L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth metrics by a large margin while maintaining performance on photometric measures.},
      eprint={2403.13206},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      selected={true}
}

@article{doi:10.1056/AIoa2300088,
  abbr={NEJM AI},
  author = {Josiah Aklilu  and Min Woo Sun  and Shelly Goel  and Sebastiano Bartoletti  and Anita Rau  and Griffin Olsen  and Kay S. Hung  and Sophie L. Mintz  and Vicki Luong  and Arnold Milstein  and Mark J. Ott  and Robert Tibshirani  and Jeffrey K. Jopling  and Eric C. Sorenson  and Dan E. Azagury  and Serena Yeung-Levy },
  title = {Artificial Intelligence Identifies Factors Associated with Blood Loss and Surgical Experience in Cholecystectomy},
  journal = {NEJM AI},
  volume = {1},
  number = {2},
  pages = {AIoa2300088},
  year = {2024},
  pdf={https://ai.nejm.org/doi/pdf/10.1056/AIoa2300088},
  URL = {https://ai.nejm.org/doi/abs/10.1056/AIoa2300088},
  eprint = {https://ai.nejm.org/doi/pdf/10.1056/AIoa2300088},
  abstract = { A computer vision model developed to recognize fine-grained surgical activity in laparoscopic cholecystectomy videos reveals associations among surgical behaviors, adverse outcomes, and surgical skills. },
  code={https://github.com/josaklil-ai/lapnet},
  selected={true}
}

@InProceedings{pmlr-v182-aklilu22a,
  abbr={MLHC},
  title={ALGES: Active Learning with Gradient Embeddings for Semantic Segmentation of Laparoscopic Surgical Images},
  author={Josiah Aklilu and Serena Yeung-Levy},
  booktitle={Proceedings of the 7th Machine Learning for Healthcare Conference},
  pages={892--911},
  year={2022},
  editor={Lipton, Zachary and Ranganath, Rajesh and Sendak, Mark and Sjoding, Michael and Yeung, Serena},
  volume={182},
  series={Proceedings of Machine Learning Research},
  month={Aug},
  publisher={PMLR},
  pdf={https://proceedings.mlr.press/v182/aklilu22a/aklilu22a.pdf},
  url={https://proceedings.mlr.press/v182/aklilu22a.html},
  abstract={Annotating medical images for the purposes of training computer vision models is an extremely laborious task that takes time and resources away from expert clinicians. Active learning (AL) is a machine learning paradigm that mitigates this problem by deliberately proposing data points that should be labeled in order to maximize model performance. We propose a novel AL algorithm for segmentation, ALGES, that utilizes gradient embeddings to effectively select laparoscopic images to be labeled by some external oracle while reducing annotation effort. Given any unlabeled image, our algorithm treats predicted segmentations as truth and computes gradients with respect to the model parameters of the last layer in a segmentation network. The norms of these per-pixel gradient vectors correspond to the magnitude of the induced change in model parameters and contain rich information about the model’s predictive uncertainty. Our algorithm then computes gradients embeddings in two ways, and we employ a center-finding algorithm with these embeddings to procure representative and diverse batches in each round of AL. An advantage of our approach is extensibility to any model architecture and differentiable loss scheme for semantic segmentation. We apply our approach to a public data set of laparoscopic cholecystectomy images and show that it outperforms current AL algorithms in selecting the most informative data points for improving the segmentation model. Our code is available at https://github.com/josaklil-ai/surg-active-learning.},
  code={https://github.com/josaklil-ai/surg-active-learning},
  selected={true}
}