---
---
@misc{aklilu2024zeroshotactionlocalizationconfidence,
  abbr={arXiv},
  title={Zero-shot Action Localization via the Confidence of Large Vision-Language Models}, 
  author={Josiah Aklilu and Xiaohan Wang and Serena Yeung-Levy},
  year={2024},
  eprint={2410.14340},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2410.14340},
  pdf={https://arxiv.org/pdf/2410.14340.pdf},
  abstract={Precise action localization in untrimmed video is vital for fields such as professional sports and minimally invasive surgery, where the delineation of particular motions in recordings can dramatically enhance analysis. But in many cases, large scale datasets with video-label pairs for localization are unavailable, limiting the opportunity to fine-tune video-understanding models. Recent developments in large vision-language models (LVLM) address this need with impressive zero-shot capabilities in a variety of video understanding tasks. However, the adaptation of image-based LVLMs, with their powerful visual question answering capabilities, to action localization in long-form video is still relatively unexplored. To this end, we introduce a true ZEro-shot Action Localization method (ZEAL). Specifically, we leverage the built-in action knowledge of a large language model (LLM) to inflate actions into highly-detailed descriptions of the archetypal start and end of the action. These descriptions serve as queries to LVLM for generating frame-level confidence scores which can be aggregated to produce localization outputs. The simplicity and flexibility of our method lends it amenable to more capable LVLMs as they are developed, and we demonstrate remarkable results in zero-shot action localization on a challenging benchmark, without any training.}, 
  selected={true}
}

@article{AutoConverter,
  abbr={arXiv},
  title={Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation},
  author={Yuhui Zhang and Yuchang Su and Yiming Liu and Xiaohan Wang and James Burgess and Elaine Sui and Chenyu Wang and Josiah Aklilu and Alejandro Lozano and Anjiang Wei and Ludwig Schmidt and Serena Yeung-Levy},
  eprint={2501.03225},
  year={2025},
  url={https://arxiv.org/abs/2501.03225},
  pdf={https://arxiv.org/pdf/2501.03225.pdf},
  abstract={The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 28 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation.},
  selected={true}
}

@misc{lozano2025biomedicaopenbiomedicalimagecaption,
  abbr={arXiv},
  title={BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature}, 
  author={Alejandro Lozano and Min Woo Sun and James Burgess and Liangyu Chen and Jeffrey J Nirschl and Jeffrey Gu and Ivan Lopez and Josiah Aklilu and Austin Wolfgang Katzer and Collin Chiu and Anita Rau and Xiaohan Wang and Yuhui Zhang and Alfred Seunghoon Song and Robert Tibshirani and Serena Yeung-Levy},
  year={2025},
  eprint={2501.07171},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2501.07171}, 
  pdf={https://arxiv.org/pdf/2501.07171.pdf},
  abstract={he development of vision-language models (VLMs) is driven by large-scale and diverse multimodal datasets. However, progress toward generalist biomedical VLMs is limited by the lack of annotated, publicly accessible datasets across biology and medicine. Existing efforts are restricted to narrow domains, missing the full diversity of biomedical knowledge encoded in scientific literature. To address this gap, we introduce BIOMEDICA, a scalable, open-source framework to extract, annotate, and serialize the entirety of the PubMed Central Open Access subset into an easy-to-use, publicly accessible this http URL framework produces a comprehensive archive with over 24 million unique image-text pairs from over 6 million articles. Metadata and expert-guided annotations are also provided. We demonstrate the utility and accessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style models continuously pre-trained on the BIOMEDICA dataset via streaming, eliminating the need to download 27 TB of data this http URL average, our models achieve state-of-the-art performance across 40 tasks - spanning pathology, radiology, ophthalmology, dermatology, surgery, molecular biology, parasitology, and cell biology - excelling in zero-shot classification with a 6.56% average improvement (as high as 29.8% and 17.5% in dermatology and ophthalmology, respectively), and stronger image-text retrieval, all while using 10x less compute. To foster reproducibility and collaboration, we release our codebase and dataset for the broader research community.},
  selected={true}
}

@article{gupte2024revisiting,
  abbr={TMLR},
  title={Revisiting Active Learning in the Era of Vision Foundation Models},
  author={Sanket Rajan Gupte* and Josiah Aklilu* and Jeffrey J Nirschl and Serena Yeung-Levy},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2024},
  url={https://openreview.net/forum?id=u8K83M9mbG},
  note={},
  pdf={https://arxiv.org/pdf/2401.14555.pdf},
  abstract={Foundation vision or vision-language models are trained on large unlabeled or noisy data and learn robust representations that can achieve impressive zero- or few-shot performance on diverse tasks. Given these properties, they are a natural fit for active learning (AL), which aims to maximize labeling efficiency, but the full potential of foundation models has not been explored in the context of AL, specifically in the low-budget regime. In this work, we evaluate how foundation models influence three critical components of effective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling, and 3) the trade-off between representative and uncertainty sampling. We systematically study how the robust representations of foundation models (DINOv2, OpenCLIP) challenge existing findings in active learning. Our observations inform the principled construction of a new simple and elegant AL strategy that balances uncertainty estimated via dropout with sample diversity. We extensively test our strategy on many challenging image classification benchmarks, including natural images as well as out-of-domain biomedical images that are relatively understudied in the AL literature. Source code will be made available.},
  eprint={2401.14555},
  code={https://github.com/sanketx/AL-foundation-models},
  selected={true}
}

@article{rau2024depthguided,
  abbr={ECCV},
  title={Depth-guided NeRF Training via Earth Mover's Distance}, 
  author={Anita Rau and Josiah Aklilu and F. Christopher Holsinger and Serena Yeung-Levy},
  journal={European Conference on Computer Vision},
  year={2024},
  note={},
  pdf={https://arxiv.org/pdf/2403.13206v1.pdf},
  abstract={Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of predicted viewpoints. However, the photometric loss often does not provide enough information to disambiguate between different possible geometries yielding the same image. Previous work has thus incorporated depth supervision during NeRF training, leveraging dense predictions from pre-trained depth networks as pseudo-ground truth. While these depth priors are assumed to be perfect once filtered for noise, in practice, their accuracy is more challenging to capture. This work proposes a novel approach to uncertainty in depth priors for NeRF supervision. Instead of using custom-trained depth or uncertainty priors, we use off-the-shelf pretrained diffusion models to predict depth and capture uncertainty during the denoising process. Because we know that depth priors are prone to errors, we propose to supervise the ray termination distance distribution with Earth Moverâ€™s Distance instead of enforcing the rendered depth to replicate the depth prior exactly through L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth metrics by a large margin while maintaining performance on photometric measures.},
  eprint={2403.13206},
  code={https://anitarau.github.io/emd-nerf.github.io/},
  selected={true}
}

@article{doi:10.1056/AIoa2300088,
  abbr={NEJM AI},
  author = {Josiah Aklilu  and Min Woo Sun  and Shelly Goel  and Sebastiano Bartoletti  and Anita Rau  and Griffin Olsen  and Kay S. Hung  and Sophie L. Mintz  and Vicki Luong  and Arnold Milstein  and Mark J. Ott  and Robert Tibshirani  and Jeffrey K. Jopling  and Eric C. Sorenson  and Dan E. Azagury  and Serena Yeung-Levy },
  title = {Artificial Intelligence Identifies Factors Associated with Blood Loss and Surgical Experience in Cholecystectomy},
  journal = {New England Journal of Medicine AI},
  volume = {1},
  number = {2},
  pages = {AIoa2300088},
  year = {2024},
  pdf={https://ai.nejm.org/doi/pdf/10.1056/AIoa2300088},
  URL = {https://ai.nejm.org/doi/abs/10.1056/AIoa2300088},
  eprint = {https://ai.nejm.org/doi/pdf/10.1056/AIoa2300088},
  abstract = { A computer vision model developed to recognize fine-grained surgical activity in laparoscopic cholecystectomy videos reveals associations among surgical behaviors, adverse outcomes, and surgical skills. },
  code={https://github.com/josaklil-ai/lapnet},
  selected={true}
}

@InProceedings{pmlr-v182-aklilu22a,
  abbr={MLHC},
  title={ALGES: Active Learning with Gradient Embeddings for Semantic Segmentation of Laparoscopic Surgical Images},
  author={Josiah Aklilu and Serena Yeung-Levy},
  booktitle={Proceedings of the 7th Machine Learning for Healthcare Conference},
  pages={892--911},
  year={2022},
  editor={Lipton, Zachary and Ranganath, Rajesh and Sendak, Mark and Sjoding, Michael and Yeung, Serena},
  volume={182},
  series={Proceedings of Machine Learning Research},
  publisher={PMLR},
  pdf={https://proceedings.mlr.press/v182/aklilu22a/aklilu22a.pdf},
  url={https://proceedings.mlr.press/v182/aklilu22a.html},
  abstract={Annotating medical images for the purposes of training computer vision models is an extremely laborious task that takes time and resources away from expert clinicians. Active learning (AL) is a machine learning paradigm that mitigates this problem by deliberately proposing data points that should be labeled in order to maximize model performance. We propose a novel AL algorithm for segmentation, ALGES, that utilizes gradient embeddings to effectively select laparoscopic images to be labeled by some external oracle while reducing annotation effort. Given any unlabeled image, our algorithm treats predicted segmentations as truth and computes gradients with respect to the model parameters of the last layer in a segmentation network. The norms of these per-pixel gradient vectors correspond to the magnitude of the induced change in model parameters and contain rich information about the modelâ€™s predictive uncertainty. Our algorithm then computes gradients embeddings in two ways, and we employ a center-finding algorithm with these embeddings to procure representative and diverse batches in each round of AL. An advantage of our approach is extensibility to any model architecture and differentiable loss scheme for semantic segmentation. We apply our approach to a public data set of laparoscopic cholecystectomy images and show that it outperforms current AL algorithms in selecting the most informative data points for improving the segmentation model. Our code is available at https://github.com/josaklil-ai/surg-active-learning.},
  code={https://github.com/josaklil-ai/surg-active-learning},
  selected={true}
}